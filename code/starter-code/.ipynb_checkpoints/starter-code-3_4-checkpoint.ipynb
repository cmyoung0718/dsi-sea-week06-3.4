{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison Lab\n",
    "\n",
    "In this lab we will compare the performance of all the models we have learned about so far, using the car evaluation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare the data\n",
    "\n",
    "The [car evaluation dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/car/) is in the assets/datasets folder. By now you should be very familiar with this dataset.\n",
    "\n",
    "1. Load the data into a pandas dataframe\n",
    "- Encode the categorical features properly: define a map that preserves the scale (assigning smaller numbers to words indicating smaller quantities)\n",
    "- Separate features from target into X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>buying</th>\n",
       "      <th>maint</th>\n",
       "      <th>doors</th>\n",
       "      <th>persons</th>\n",
       "      <th>lug_boot</th>\n",
       "      <th>safety</th>\n",
       "      <th>acceptability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>med</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>high</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>med</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>med</td>\n",
       "      <td>med</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  buying  maint doors persons lug_boot safety acceptability\n",
       "0  vhigh  vhigh     2       2    small    low         unacc\n",
       "1  vhigh  vhigh     2       2    small    med         unacc\n",
       "2  vhigh  vhigh     2       2    small   high         unacc\n",
       "3  vhigh  vhigh     2       2      med    low         unacc\n",
       "4  vhigh  vhigh     2       2      med    med         unacc"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./../../assets/datasets/car.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>buying_high</th>\n",
       "      <th>buying_low</th>\n",
       "      <th>buying_med</th>\n",
       "      <th>buying_vhigh</th>\n",
       "      <th>maint_high</th>\n",
       "      <th>maint_low</th>\n",
       "      <th>maint_med</th>\n",
       "      <th>maint_vhigh</th>\n",
       "      <th>doors_2</th>\n",
       "      <th>doors_3</th>\n",
       "      <th>...</th>\n",
       "      <th>doors_5more</th>\n",
       "      <th>persons_2</th>\n",
       "      <th>persons_4</th>\n",
       "      <th>persons_more</th>\n",
       "      <th>lug_boot_big</th>\n",
       "      <th>lug_boot_med</th>\n",
       "      <th>lug_boot_small</th>\n",
       "      <th>safety_high</th>\n",
       "      <th>safety_low</th>\n",
       "      <th>safety_med</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   buying_high  buying_low  buying_med  buying_vhigh  maint_high  maint_low  \\\n",
       "0          0.0         0.0         0.0           1.0         0.0        0.0   \n",
       "1          0.0         0.0         0.0           1.0         0.0        0.0   \n",
       "2          0.0         0.0         0.0           1.0         0.0        0.0   \n",
       "3          0.0         0.0         0.0           1.0         0.0        0.0   \n",
       "4          0.0         0.0         0.0           1.0         0.0        0.0   \n",
       "\n",
       "   maint_med  maint_vhigh  doors_2  doors_3     ...      doors_5more  \\\n",
       "0        0.0          1.0      1.0      0.0     ...              0.0   \n",
       "1        0.0          1.0      1.0      0.0     ...              0.0   \n",
       "2        0.0          1.0      1.0      0.0     ...              0.0   \n",
       "3        0.0          1.0      1.0      0.0     ...              0.0   \n",
       "4        0.0          1.0      1.0      0.0     ...              0.0   \n",
       "\n",
       "   persons_2  persons_4  persons_more  lug_boot_big  lug_boot_med  \\\n",
       "0        1.0        0.0           0.0           0.0           0.0   \n",
       "1        1.0        0.0           0.0           0.0           0.0   \n",
       "2        1.0        0.0           0.0           0.0           0.0   \n",
       "3        1.0        0.0           0.0           0.0           1.0   \n",
       "4        1.0        0.0           0.0           0.0           1.0   \n",
       "\n",
       "   lug_boot_small  safety_high  safety_low  safety_med  \n",
       "0             1.0          0.0         1.0         0.0  \n",
       "1             1.0          0.0         0.0         1.0  \n",
       "2             1.0          1.0         0.0         0.0  \n",
       "3             0.0          0.0         1.0         0.0  \n",
       "4             0.0          0.0         0.0         1.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.get_dummies(df.drop('acceptability', axis=1))\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df['acceptability'])\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Useful preparation\n",
    "\n",
    "Since we will compare several models, let's write a couple of helper functions.\n",
    "\n",
    "1. Separate X and y between a train and test set, using 30% test set, random state = 42\n",
    "    - make sure that the data is shuffled and stratified\n",
    "2. Define a function called `evaluate_model`, that trains the model on the train set, tests it on the test, calculates:\n",
    "    - accuracy score\n",
    "    - confusion matrix\n",
    "    - classification report\n",
    "3. Initialize a global dictionary to store the various models for later retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    # create model object\n",
    "    mod = model\n",
    "    # fit model\n",
    "    mod.fit(X_train, y_train)\n",
    "    y_pred = mod.predict(X_test)\n",
    "    print 'Accuracy score:', mod.score(X_test, y_test)\n",
    "    con = pd.DataFrame(confusion_matrix(y_pred, y_test))\n",
    "    con.columns = ['acc', 'good', 'unacc', 'vgood']\n",
    "    con.index = ['pred_acc', 'pred_good', 'pred_unacc', 'pred_vgood']\n",
    "    print con\n",
    "    print classification_report(y_pred, y_test)\n",
    "    return mod\n",
    "\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.a KNN\n",
    "\n",
    "Let's start with `KNeighborsClassifier`.\n",
    "\n",
    "1. Initialize a KNN model\n",
    "- Evaluate it's performance with the function you previously defined\n",
    "- Find the optimal value of K using grid search\n",
    "    - Be careful on how you perform the cross validation in the grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.894921190893\n",
      "            acc  good  unacc  vgood\n",
      "pred_acc     99    12      9      2\n",
      "pred_good     3     8      0      2\n",
      "pred_unacc   25     2    391      4\n",
      "pred_vgood    0     1      0     13\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.81      0.80       122\n",
      "          1       0.35      0.62      0.44        13\n",
      "          2       0.98      0.93      0.95       422\n",
      "          3       0.62      0.93      0.74        14\n",
      "\n",
      "avg / total       0.91      0.89      0.90       571\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models['KNN'] = evaluate_model(KNeighborsClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 8}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pg = {'n_neighbors': [i for i in range(1,20)]}\n",
    "gs = GridSearchCV(models['KNN'], param_grid=pg, cv=5)\n",
    "gs.fit(X_train, y_train)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.b Bagging + KNN\n",
    "\n",
    "Now that we have found the optimal K, let's wrap `KNeighborsClassifier` in a BaggingClassifier and see if the score improves.\n",
    "\n",
    "1. Wrap the KNN model in a Bagging Classifier\n",
    "- Evaluate performance\n",
    "- Do a grid search only on the bagging classifier params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.907180385289\n",
      "            acc  good  unacc  vgood\n",
      "pred_acc    100    12      8      4\n",
      "pred_good     0    11      0      2\n",
      "pred_unacc   27     0    392      0\n",
      "pred_vgood    0     0      0     15\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.81      0.80       124\n",
      "          1       0.48      0.85      0.61        13\n",
      "          2       0.98      0.94      0.96       419\n",
      "          3       0.71      1.00      0.83        15\n",
      "\n",
      "avg / total       0.92      0.91      0.91       571\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models['bagging_knn'] = evaluate_model(BaggingClassifier(models['KNN'].set_params(n_neighbors=8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 4, 'bootstrap': False, 'bootstrap_features': False}\n"
     ]
    }
   ],
   "source": [
    "pg = {'n_estimators': [4, 8, 10, 12, 15],\n",
    "     'bootstrap': [True, False],\n",
    "     'bootstrap_features': [True, False]}\n",
    "def grid_search(model, pg):\n",
    "    gs = GridSearchCV(model, param_grid=pg, cv=5)\n",
    "    gs.fit(X_train, y_train)\n",
    "    print gs.best_params_\n",
    "grid_search(models['bagging_knn'], pg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.910683012259\n",
      "            acc  good  unacc  vgood\n",
      "pred_acc    112    11     16      4\n",
      "pred_good     2    11      0      2\n",
      "pred_unacc   13     1    384      2\n",
      "pred_vgood    0     0      0     13\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.78      0.83       143\n",
      "          1       0.48      0.73      0.58        15\n",
      "          2       0.96      0.96      0.96       400\n",
      "          3       0.62      1.00      0.76        13\n",
      "\n",
      "avg / total       0.92      0.91      0.91       571\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=8, p=2,\n",
       "           weights='uniform'),\n",
       "         bootstrap=False, bootstrap_features=False, max_features=1.0,\n",
       "         max_samples=1.0, n_estimators=4, n_jobs=1, oob_score=False,\n",
       "         random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(models['bagging_knn'].set_params(bootstrap=False, bootstrap_features=False, n_estimators=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Logistic Regression\n",
    "\n",
    "Let's see if logistic regression performs better\n",
    "\n",
    "1. Initialize LR and test on Train/Test set\n",
    "- Find optimal params with Grid Search\n",
    "- See if Bagging improves the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.879159369527\n",
      "            acc  good  unacc  vgood\n",
      "pred_acc    102    16     16     11\n",
      "pred_good     4     6      0      0\n",
      "pred_unacc   21     0    384      0\n",
      "pred_vgood    0     1      0     10\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.70      0.75       145\n",
      "          1       0.26      0.60      0.36        10\n",
      "          2       0.96      0.95      0.95       405\n",
      "          3       0.48      0.91      0.62        11\n",
      "\n",
      "avg / total       0.90      0.88      0.89       571\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models['logreg'] = evaluate_model(LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'penalty': 'l1', 'C': 1000.0}\n"
     ]
    }
   ],
   "source": [
    "pg = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0, 10000.0]\n",
    "}\n",
    "grid_search(models['logreg'], pg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.893169877408\n",
      "            acc  good  unacc  vgood\n",
      "pred_acc    100    14     16      4\n",
      "pred_good     7     9      0      0\n",
      "pred_unacc   17     0    384      0\n",
      "pred_vgood    3     0      0     17\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.75      0.77       134\n",
      "          1       0.39      0.56      0.46        16\n",
      "          2       0.96      0.96      0.96       401\n",
      "          3       0.81      0.85      0.83        20\n",
      "\n",
      "avg / total       0.90      0.89      0.90       571\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models['bagging_logreg'] = evaluate_model(BaggingClassifier(models['logreg'].set_params(penalty='l1', C=1000.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 12, 'bootstrap': True, 'bootstrap_features': False}\n"
     ]
    }
   ],
   "source": [
    "pg = {'n_estimators': [4, 8, 10, 12, 15],\n",
    "     'bootstrap': [True, False],\n",
    "     'bootstrap_features': [True, False]}\n",
    "grid_search(models['bagging_logreg'], pg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.900175131349\n",
      "            acc  good  unacc  vgood\n",
      "pred_acc    101    14     14      3\n",
      "pred_good     6     9      0      0\n",
      "pred_unacc   17     0    386      0\n",
      "pred_vgood    3     0      0     18\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.77      0.78       132\n",
      "          1       0.39      0.60      0.47        15\n",
      "          2       0.96      0.96      0.96       403\n",
      "          3       0.86      0.86      0.86        21\n",
      "\n",
      "avg / total       0.91      0.90      0.90       571\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=LogisticRegression(C=1000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False),\n",
       "         bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
       "         max_samples=1.0, n_estimators=12, n_jobs=1, oob_score=False,\n",
       "         random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(models['bagging_logreg'].set_params(n_estimators=12, bootstrap=True, bootstrap_features=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Decision Trees\n",
    "\n",
    "Let's see if Decision Trees perform better\n",
    "\n",
    "1. Initialize DT and test on Train/Test set\n",
    "- Find optimal params with Grid Search\n",
    "- See if Bagging improves the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.957968476357\n",
      "            acc  good  unacc  vgood\n",
      "pred_acc    121     5     10      1\n",
      "pred_good     2    18      0      2\n",
      "pred_unacc    4     0    390      0\n",
      "pred_vgood    0     0      0     18\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.88      0.92       137\n",
      "          1       0.78      0.82      0.80        22\n",
      "          2       0.97      0.99      0.98       394\n",
      "          3       0.86      1.00      0.92        18\n",
      "\n",
      "avg / total       0.96      0.96      0.96       571\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models['dec_tree'] = evaluate_model(DecisionTreeClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': None, 'max_leaf_nodes': None, 'criterion': 'entropy', 'max_depth': None}\n"
     ]
    }
   ],
   "source": [
    "pg = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_features': [None, 1, 2, 3],\n",
    "    'max_depth': [None, 4, 5, 6, 7, 8, 9],\n",
    "    'max_leaf_nodes': [None, 4, 5, 6, 7, 8, 9]\n",
    "}\n",
    "grid_search(models['dec_tree'], pg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.968476357268\n",
      "            acc  good  unacc  vgood\n",
      "pred_acc    120     3      7      1\n",
      "pred_good     1    20      0      0\n",
      "pred_unacc    6     0    393      0\n",
      "pred_vgood    0     0      0     20\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.92      0.93       131\n",
      "          1       0.87      0.95      0.91        21\n",
      "          2       0.98      0.98      0.98       399\n",
      "          3       0.95      1.00      0.98        20\n",
      "\n",
      "avg / total       0.97      0.97      0.97       571\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models['dec_tree'] = evaluate_model(models['dec_tree'].set_params(criterion='entropy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.977232924694\n",
      "            acc  good  unacc  vgood\n",
      "pred_acc    123     1      7      0\n",
      "pred_good     0    22      0      1\n",
      "pred_unacc    3     0    393      0\n",
      "pred_vgood    1     0      0     20\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.94      0.95       131\n",
      "          1       0.96      0.96      0.96        23\n",
      "          2       0.98      0.99      0.99       396\n",
      "          3       0.95      0.95      0.95        21\n",
      "\n",
      "avg / total       0.98      0.98      0.98       571\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(BaggingClassifier(models['dec_tree']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 10, 'bootstrap': False, 'bootstrap_features': False}\n"
     ]
    }
   ],
   "source": [
    "pg = {'n_estimators': [4, 8, 10, 12, 15],\n",
    "     'bootstrap': [True, False],\n",
    "     'bootstrap_features': [True, False]}\n",
    "grid_search(models['bagging_dec_tree'], pg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.977232924694\n",
      "            acc  good  unacc  vgood\n",
      "pred_acc    122     0      7      1\n",
      "pred_good     2    23      0      0\n",
      "pred_unacc    3     0    393      0\n",
      "pred_vgood    0     0      0     20\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.94      0.95       130\n",
      "          1       1.00      0.92      0.96        25\n",
      "          2       0.98      0.99      0.99       396\n",
      "          3       0.95      1.00      0.98        20\n",
      "\n",
      "avg / total       0.98      0.98      0.98       571\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models['bagging_dec_tree'] = evaluate_model(BaggingClassifier(models['dec_tree'], n_estimators=10, \n",
    "                                                              bootstrap=False, bootstrap_features=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Support Vector Machines\n",
    "\n",
    "Let's see if SVM perform better\n",
    "\n",
    "1. Initialize SVM and test on Train/Test set\n",
    "- Find optimal params with Grid Search\n",
    "- See if Bagging improves the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.903677758319\n",
      "            acc  good  unacc  vgood\n",
      "pred_acc    124    22     17     12\n",
      "pred_good     0     0      0      0\n",
      "pred_unacc    3     0    383      0\n",
      "pred_vgood    0     1      0      9\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.71      0.82       175\n",
      "          1       0.00      0.00      0.00         0\n",
      "          2       0.96      0.99      0.97       386\n",
      "          3       0.43      0.90      0.58        10\n",
      "\n",
      "avg / total       0.95      0.90      0.92       571\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models['svc'] = evaluate_model(SVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kernel': 'poly', 'C': 1000.0, 'degree': 3}\n"
     ]
    }
   ],
   "source": [
    "pg = {\n",
    "    'C': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0, 10000.0],\n",
    "    'kernel': ['rbf', 'linear', 'poly'],\n",
    "    'degree': [1, 2, 3, 4, 5]\n",
    "}\n",
    "grid_search(models['svc'], pg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.998248686515\n",
      "            acc  good  unacc  vgood\n",
      "pred_acc    127     1      0      0\n",
      "pred_good     0    22      0      0\n",
      "pred_unacc    0     0    400      0\n",
      "pred_vgood    0     0      0     21\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.99      1.00       128\n",
      "          1       0.96      1.00      0.98        22\n",
      "          2       1.00      1.00      1.00       400\n",
      "          3       1.00      1.00      1.00        21\n",
      "\n",
      "avg / total       1.00      1.00      1.00       571\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models['svc'] = evaluate_model(SVC(kernel='poly', C=1000.0, degree=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.991243432574\n",
      "            acc  good  unacc  vgood\n",
      "pred_acc    127     3      1      1\n",
      "pred_good     0    20      0      0\n",
      "pred_unacc    0     0    399      0\n",
      "pred_vgood    0     0      0     20\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.96      0.98       132\n",
      "          1       0.87      1.00      0.93        20\n",
      "          2       1.00      1.00      1.00       399\n",
      "          3       0.95      1.00      0.98        20\n",
      "\n",
      "avg / total       0.99      0.99      0.99       571\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models['bagging_svc'] = evaluate_model(BaggingClassifier(models['svc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 4, 'bootstrap': False, 'bootstrap_features': False}\n"
     ]
    }
   ],
   "source": [
    "pg = {'n_estimators': [4, 8, 10, 12, 15],\n",
    "     'bootstrap': [True, False],\n",
    "     'bootstrap_features': [True, False]}\n",
    "grid_search(models['bagging_svc'], pg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.998248686515\n",
      "            acc  good  unacc  vgood\n",
      "pred_acc    127     1      0      0\n",
      "pred_good     0    22      0      0\n",
      "pred_unacc    0     0    400      0\n",
      "pred_vgood    0     0      0     21\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.99      1.00       128\n",
      "          1       0.96      1.00      0.98        22\n",
      "          2       1.00      1.00      1.00       400\n",
      "          3       1.00      1.00      1.00        21\n",
      "\n",
      "avg / total       1.00      1.00      1.00       571\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models['bagging_svc'] = evaluate_model(BaggingClassifier(models['svc'], n_estimators=4, \n",
    "                                                              bootstrap=False, bootstrap_features=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Random Forest & Extra Trees\n",
    "\n",
    "Let's see if Random Forest and Extra Trees perform better\n",
    "\n",
    "1. Initialize RF and ET and test on Train/Test set\n",
    "- Find optimal params with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.933450087566\n",
      "            acc  good  unacc  vgood\n",
      "pred_acc    117    11     11      3\n",
      "pred_good     0    10      0      1\n",
      "pred_unacc    9     0    389      0\n",
      "pred_vgood    1     2      0     17\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.82      0.87       142\n",
      "          1       0.43      0.91      0.59        11\n",
      "          2       0.97      0.98      0.97       398\n",
      "          3       0.81      0.85      0.83        20\n",
      "\n",
      "avg / total       0.94      0.93      0.94       571\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models['rand_for'] = evaluate_model(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': False, 'criterion': 'entropy', 'max_depth': None}\n"
     ]
    }
   ],
   "source": [
    "pg = {\n",
    "    'bootstrap': [True, False],\n",
    "    'criterion':['gini', 'entropy'],\n",
    "    'max_depth': [None, 3, 5, 7, 9, 15]\n",
    "}\n",
    "grid_search(models['rand_for'], pg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.947460595447\n",
      "            acc  good  unacc  vgood\n",
      "pred_acc    115     5     10      2\n",
      "pred_good     2    18      0      1\n",
      "pred_unacc    9     0    390      0\n",
      "pred_vgood    1     0      0     18\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.87      0.89       132\n",
      "          1       0.78      0.86      0.82        21\n",
      "          2       0.97      0.98      0.98       399\n",
      "          3       0.86      0.95      0.90        19\n",
      "\n",
      "avg / total       0.95      0.95      0.95       571\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models['rand_for'] = evaluate_model(RandomForestClassifier(bootstrap=False, criterion='entropy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.956217162872\n",
      "            acc  good  unacc  vgood\n",
      "pred_acc    123     6     13      1\n",
      "pred_good     0    16      0      0\n",
      "pred_unacc    4     0    387      0\n",
      "pred_vgood    0     1      0     20\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.86      0.91       143\n",
      "          1       0.70      1.00      0.82        16\n",
      "          2       0.97      0.99      0.98       391\n",
      "          3       0.95      0.95      0.95        21\n",
      "\n",
      "avg / total       0.96      0.96      0.96       571\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models['et'] = evaluate_model(ExtraTreesClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': False, 'criterion': 'entropy', 'max_depth': 15}\n"
     ]
    }
   ],
   "source": [
    "pg = {\n",
    "    'bootstrap': [True, False],\n",
    "    'criterion':['gini', 'entropy'],\n",
    "    'max_depth': [None, 3, 5, 7, 9, 15]\n",
    "}\n",
    "grid_search(models['et'], pg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.952714535902\n",
      "            acc  good  unacc  vgood\n",
      "pred_acc    118     4     13      1\n",
      "pred_good     0    19      0      0\n",
      "pred_unacc    8     0    387      0\n",
      "pred_vgood    1     0      0     20\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.87      0.90       136\n",
      "          1       0.83      1.00      0.90        19\n",
      "          2       0.97      0.98      0.97       395\n",
      "          3       0.95      0.95      0.95        21\n",
      "\n",
      "avg / total       0.95      0.95      0.95       571\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models['et'] = evaluate_model(ExtraTreesClassifier(bootstrap=False, criterion='entropy', max_depth=15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model comparison\n",
    "\n",
    "Let's compare the scores of the various models.\n",
    "\n",
    "1. Do a bar chart of the scores of the best models. Who's the winner on the train/test split?\n",
    "- Re-test all the models using a 3 fold stratified shuffled cross validation\n",
    "- Do a bar chart with errorbars of the cross validation average scores. is the winner the same?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "\n",
    "We have encoded the data using a map that preserves the scale.\n",
    "Would our results have changed if we had encoded the categorical data using `pd.get_dummies` or `OneHotEncoder`  to encode them as binary variables instead?\n",
    "\n",
    "1. Repeat the analysis for this scenario. Is it better?\n",
    "- Experiment with other models or other parameters, can you beat your classmates best score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
