{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison Lab\n",
    "\n",
    "In this lab we will compare the performance of all the models we have learned about so far, using the car evaluation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare the data\n",
    "\n",
    "The [car evaluation dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/car/) is in the assets/datasets folder. By now you should be very familiar with this dataset.\n",
    "\n",
    "1. Load the data into a pandas dataframe\n",
    "- Encode the categorical features properly: define a map that preserves the scale (assigning smaller numbers to words indicating smaller quantities)\n",
    "- Separate features from target into X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./../../assets/datasets/car.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['safety'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "map_maint = {'vhigh': 4,\n",
    "       'high': 3,\n",
    "       'med': 2,\n",
    "       'low':1}\n",
    "map_doors = {'5more': 4,\n",
    "       '4': 3,\n",
    "       '3': 2,\n",
    "       '2':1}\n",
    "map_persons = {'more': 3,\n",
    "       '4': 2,\n",
    "       '2':1}\n",
    "map_lug_boot = {'big': 3,\n",
    "       'med': 2,\n",
    "       'small':1}\n",
    "map_safety = {'high': 3,\n",
    "       'med': 2,\n",
    "       'low':1}\n",
    "\n",
    "\n",
    "# def thing(i):return map_maint[i]\n",
    "X_ = pd.DataFrame()\n",
    "X_['maint'] = map(lambda(x):map_maint[x], df['maint'])\n",
    "X_['buying'] = map(lambda(x):map_maint[x], df['buying'])\n",
    "X_['doors'] = map(lambda(x):map_doors[x], df['doors'])\n",
    "X_['persons'] = map(lambda(x):map_persons[x], df['persons'])\n",
    "X_['lug_boot'] = map(lambda(x):map_lug_boot[x], df['lug_boot'])\n",
    "X_['safety'] = map(lambda(x):map_safety[x], df['safety'])\n",
    "X = X_\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X = pd.get_dummies(df.drop('acceptability', axis=1))\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df['acceptability'])\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Useful preparation\n",
    "\n",
    "Since we will compare several models, let's write a couple of helper functions.\n",
    "\n",
    "1. Separate X and y between a train and test set, using 30% test set, random state = 42\n",
    "    - make sure that the data is shuffled and stratified\n",
    "2. Define a function called `evaluate_model`, that trains the model on the train set, tests it on the test, calculates:\n",
    "    - accuracy score\n",
    "    - confusion matrix\n",
    "    - classification report\n",
    "3. Initialize a global dictionary to store the various models for later retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    # create model object\n",
    "    mod = model\n",
    "    # fit model\n",
    "    mod.fit(X_train, y_train)\n",
    "    y_pred = mod.predict(X_test)\n",
    "    print 'Accuracy score:', mod.score(X_test, y_test)\n",
    "    con = pd.DataFrame(confusion_matrix(y_pred, y_test))\n",
    "    con.columns = ['acc', 'good', 'unacc', 'vgood']\n",
    "    con.index = ['pred_acc', 'pred_good', 'pred_unacc', 'pred_vgood']\n",
    "    print con\n",
    "    print classification_report(y_pred, y_test)\n",
    "    return mod\n",
    "\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.a KNN\n",
    "\n",
    "Let's start with `KNeighborsClassifier`.\n",
    "\n",
    "1. Initialize a KNN model\n",
    "- Evaluate it's performance with the function you previously defined\n",
    "- Find the optimal value of K using grid search\n",
    "    - Be careful on how you perform the cross validation in the grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models['KNN'] = evaluate_model(KNeighborsClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pg = {'n_neighbors': [i for i in range(1,20)]}\n",
    "gs = GridSearchCV(models['KNN'], param_grid=pg, cv=5)\n",
    "gs.fit(X_train, y_train)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.b Bagging + KNN\n",
    "\n",
    "Now that we have found the optimal K, let's wrap `KNeighborsClassifier` in a BaggingClassifier and see if the score improves.\n",
    "\n",
    "1. Wrap the KNN model in a Bagging Classifier\n",
    "- Evaluate performance\n",
    "- Do a grid search only on the bagging classifier params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models['bagging_knn'] = evaluate_model(BaggingClassifier(models['KNN'].set_params(n_neighbors=5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pg = {'n_estimators': [4, 8, 10, 12, 15],\n",
    "     'bootstrap': [True, False],\n",
    "     'bootstrap_features': [True, False]}\n",
    "def grid_search(model, pg):\n",
    "    gs = GridSearchCV(model, param_grid=pg, cv=3)\n",
    "    gs.fit(X_train, y_train)\n",
    "    print gs.best_params_\n",
    "grid_search(models['bagging_knn'], pg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "evaluate_model(models['bagging_knn'].set_params(bootstrap=True, bootstrap_features=False, n_estimators=15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Logistic Regression\n",
    "\n",
    "Let's see if logistic regression performs better\n",
    "\n",
    "1. Initialize LR and test on Train/Test set\n",
    "- Find optimal params with Grid Search\n",
    "- See if Bagging improves the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models['logreg'] = evaluate_model(LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pg = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0, 10000.0]\n",
    "}\n",
    "grid_search(models['logreg'], pg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models['bagging_logreg'] = evaluate_model(BaggingClassifier(models['logreg'].set_params(penalty='l1', C=100.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pg = {'n_estimators': [4, 8, 10, 12, 15],\n",
    "     'bootstrap': [True, False],\n",
    "     'bootstrap_features': [True, False]}\n",
    "grid_search(models['bagging_logreg'], pg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluate_model(models['bagging_logreg'].set_params(n_estimators=10, bootstrap=True, bootstrap_features=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Decision Trees\n",
    "\n",
    "Let's see if Decision Trees perform better\n",
    "\n",
    "1. Initialize DT and test on Train/Test set\n",
    "- Find optimal params with Grid Search\n",
    "- See if Bagging improves the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models['dec_tree'] = evaluate_model(DecisionTreeClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pg = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_features': [None, 1, 2, 3],\n",
    "    'max_depth': [None, 4, 5, 6, 7, 8, 9],\n",
    "    'max_leaf_nodes': [None, 4, 5, 6, 7, 8, 9]\n",
    "}\n",
    "grid_search(models['dec_tree'], pg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models['dec_tree'] = evaluate_model(models['dec_tree'].set_params(criterion='entropy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models['bagging_dec_tree'] = evaluate_model(BaggingClassifier(models['dec_tree']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pg = {'n_estimators': [4, 8, 10, 12, 15],\n",
    "     'bootstrap': [True, False],\n",
    "     'bootstrap_features': [True, False]}\n",
    "grid_search(models['bagging_dec_tree'], pg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models['bagging_dec_tree'] = evaluate_model(BaggingClassifier(models['dec_tree'], n_estimators=4, \n",
    "                                                              bootstrap=False, bootstrap_features=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Support Vector Machines\n",
    "\n",
    "Let's see if SVM perform better\n",
    "\n",
    "1. Initialize SVM and test on Train/Test set\n",
    "- Find optimal params with Grid Search\n",
    "- See if Bagging improves the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models['svc'] = evaluate_model(SVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pg = {\n",
    "    'C': [1.0, 10.0, 100.0, 1000.0, 10000.0],\n",
    "    'kernel': ['rbf', 'linear', 'poly'],\n",
    "    'degree': [1, 2, 3, 4, 5]\n",
    "}\n",
    "grid_search(models['svc'], pg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models['svc'] = evaluate_model(SVC(kernel='poly', C=1000.0, degree=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models['bagging_svc'] = evaluate_model(BaggingClassifier(models['svc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pg = {'n_estimators': [4, 8, 10, 12, 15],\n",
    "     'bootstrap': [True, False],\n",
    "     'bootstrap_features': [True, False]}\n",
    "grid_search(models['bagging_svc'], pg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models['bagging_svc'] = evaluate_model(BaggingClassifier(models['svc'], n_estimators=4, \n",
    "                                                              bootstrap=False, bootstrap_features=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Random Forest & Extra Trees\n",
    "\n",
    "Let's see if Random Forest and Extra Trees perform better\n",
    "\n",
    "1. Initialize RF and ET and test on Train/Test set\n",
    "- Find optimal params with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models['rand_for'] = evaluate_model(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pg = {\n",
    "    'bootstrap': [True, False],\n",
    "    'criterion':['gini', 'entropy'],\n",
    "    'max_depth': [None, 3, 5, 7, 9, 15]\n",
    "}\n",
    "grid_search(models['rand_for'], pg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models['rand_for'] = evaluate_model(RandomForestClassifier(bootstrap=False, criterion='entropy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models['et'] = evaluate_model(ExtraTreesClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pg = {\n",
    "    'bootstrap': [True, False],\n",
    "    'criterion':['gini', 'entropy'],\n",
    "    'max_depth': [None, 3, 5, 7, 9, 15]\n",
    "}\n",
    "grid_search(models['et'], pg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models['et'] = evaluate_model(ExtraTreesClassifier(bootstrap=False, criterion='entropy', max_depth=15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model comparison\n",
    "\n",
    "Let's compare the scores of the various models.\n",
    "\n",
    "1. Do a bar chart of the scores of the best models. Who's the winner on the train/test split?\n",
    "- Re-test all the models using a 3 fold stratified shuffled cross validation\n",
    "- Do a bar chart with errorbars of the cross validation average scores. is the winner the same?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "\n",
    "We have encoded the data using a map that preserves the scale.\n",
    "Would our results have changed if we had encoded the categorical data using `pd.get_dummies` or `OneHotEncoder`  to encode them as binary variables instead?\n",
    "\n",
    "1. Repeat the analysis for this scenario. Is it better?\n",
    "- Experiment with other models or other parameters, can you beat your classmates best score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
